{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import libraries","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport time\nfrom torchvision import tv_tensors\nfrom torchvision.transforms import v2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-29T15:23:43.564444Z","iopub.execute_input":"2024-08-29T15:23:43.564843Z","iopub.status.idle":"2024-08-29T15:23:49.069119Z","shell.execute_reply.started":"2024-08-29T15:23:43.564805Z","shell.execute_reply":"2024-08-29T15:23:49.068091Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"class Timer:\n    \"\"\"Record multiple running times.\"\"\"\n    def __init__(self):\n        \"\"\"Defined in :numref:`sec_minibatch_sgd`\"\"\"\n        self.times = []\n        self.start()\n\n    def start(self):\n        \"\"\"Start the timer.\"\"\"\n        self.tik = time.time()\n\n    def stop(self):\n        \"\"\"Stop the timer and record the time in a list.\"\"\"\n        self.times.append(time.time() - self.tik)\n        return self.times[-1]\n\n    def avg(self):\n        \"\"\"Return the average time.\"\"\"\n        return sum(self.times) / len(self.times)\n\n    def sum(self):\n        \"\"\"Return the sum of time.\"\"\"\n        return sum(self.times)\n\n    def cumsum(self):\n        \"\"\"Return the accumulated time.\"\"\"\n        return np.array(self.times).cumsum().tolist()","metadata":{"execution":{"iopub.status.busy":"2024-08-29T15:23:49.071141Z","iopub.execute_input":"2024-08-29T15:23:49.071580Z","iopub.status.idle":"2024-08-29T15:23:49.080554Z","shell.execute_reply.started":"2024-08-29T15:23:49.071526Z","shell.execute_reply":"2024-08-29T15:23:49.079406Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"voc_trainset = torchvision.datasets.VOCDetection(\n    root='/kaggle/working/', year='2012', image_set='train', download=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-29T15:23:49.081829Z","iopub.execute_input":"2024-08-29T15:23:49.082293Z","iopub.status.idle":"2024-08-29T15:24:54.331918Z","shell.execute_reply.started":"2024-08-29T15:23:49.082241Z","shell.execute_reply":"2024-08-29T15:24:54.330548Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar to /kaggle/working/VOCtrainval_11-May-2012.tar\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1999639040/1999639040 [00:51<00:00, 39057123.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting /kaggle/working/VOCtrainval_11-May-2012.tar to /kaggle/working/\n","output_type":"stream"}]},{"cell_type":"code","source":"voc_valset = torchvision.datasets.VOCDetection(\n    root='/kaggle/working/', year='2012', image_set='val', download=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-29T15:24:54.334812Z","iopub.execute_input":"2024-08-29T15:24:54.335235Z","iopub.status.idle":"2024-08-29T15:25:21.053694Z","shell.execute_reply.started":"2024-08-29T15:24:54.335192Z","shell.execute_reply":"2024-08-29T15:25:21.052802Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Using downloaded and verified file: /kaggle/working/VOCtrainval_11-May-2012.tar\nExtracting /kaggle/working/VOCtrainval_11-May-2012.tar to /kaggle/working/\n","output_type":"stream"}]},{"cell_type":"code","source":"VOC_I2N = {0:'aeroplane',\n1:'bicycle',\n2:'bird',\n3:'boat',\n4:'bottle',\n5:'bus',\n6:'car',\n7:'cat',\n8:'chair',\n9:'cow',\n10:'diningtable',\n11:'dog',\n12:'horse',\n13:'motorbike',\n14:'person',\n15:'pottedplant',\n16:'sheep',\n17:'sofa',\n18:'train',\n19:'tvmonitor'}\nVOC_N2I = {n: i for (i, n) in zip(VOC_I2N.keys(), VOC_I2N.values())}","metadata":{"execution":{"iopub.status.busy":"2024-08-29T15:25:21.054967Z","iopub.execute_input":"2024-08-29T15:25:21.055334Z","iopub.status.idle":"2024-08-29T15:25:21.062378Z","shell.execute_reply.started":"2024-08-29T15:25:21.055298Z","shell.execute_reply":"2024-08-29T15:25:21.061291Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nbbox = lambda x: [int(x['xmin']), int(x['ymin']), int(x['xmax']), int(x['ymax'])]\ndic = {'file_name':[], 'bbox': [], 'name': []}\nfor img, annotation in tqdm(voc_trainset):\n    for obj in annotation['annotation']['object']:\n        dic['file_name'].append(annotation['annotation']['filename'])\n        dic['bbox'].append(bbox(obj['bndbox']))\n        dic['name'].append(VOC_N2I[obj['name']])","metadata":{"execution":{"iopub.status.busy":"2024-08-29T15:25:21.063745Z","iopub.execute_input":"2024-08-29T15:25:21.064093Z","iopub.status.idle":"2024-08-29T15:25:43.570995Z","shell.execute_reply.started":"2024-08-29T15:25:21.064016Z","shell.execute_reply":"2024-08-29T15:25:43.569964Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"100%|██████████| 5717/5717 [00:18<00:00, 304.76it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\ndata = pd.DataFrame(dic)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-29T15:25:43.572438Z","iopub.execute_input":"2024-08-29T15:25:43.572879Z","iopub.status.idle":"2024-08-29T15:25:44.101075Z","shell.execute_reply.started":"2024-08-29T15:25:43.572828Z","shell.execute_reply":"2024-08-29T15:25:44.100107Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"         file_name                 bbox  name\n0  2008_000008.jpg   [53, 87, 471, 420]    12\n1  2008_000008.jpg  [158, 44, 289, 167]    14\n2  2008_000015.jpg   [270, 1, 378, 176]     4\n3  2008_000015.jpg    [57, 1, 164, 150]     4\n4  2008_000019.jpg   [139, 2, 372, 197]    11","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_name</th>\n      <th>bbox</th>\n      <th>name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2008_000008.jpg</td>\n      <td>[53, 87, 471, 420]</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2008_000008.jpg</td>\n      <td>[158, 44, 289, 167]</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2008_000015.jpg</td>\n      <td>[270, 1, 378, 176]</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2008_000015.jpg</td>\n      <td>[57, 1, 164, 150]</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2008_000019.jpg</td>\n      <td>[139, 2, 372, 197]</td>\n      <td>11</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def resize_transform(image, target, max_size):\n    w, h = image.size\n    w_delta = w - max_size\n    h_delta = h - max_size\n    if w_delta > 0:\n        image, target = v2.Pad([w_delta//2, 0])(image, target)\n        w = max_size\n    else:\n        image, target = v2.CenterCrop([h, max_size])(image, target)\n    if h_delta > 0:\n        image, target = v2.Pad([0, h_delta//2])(image, target)\n        h = max_size\n    else:\n        image, target = v2.CenterCrop([max_size, h])(image, target)\n    image, target = v2.Resize((max_size, max_size))(image, target)\n    return image, target","metadata":{"execution":{"iopub.status.busy":"2024-08-29T15:25:44.102329Z","iopub.execute_input":"2024-08-29T15:25:44.102817Z","iopub.status.idle":"2024-08-29T15:25:44.112089Z","shell.execute_reply.started":"2024-08-29T15:25:44.102778Z","shell.execute_reply":"2024-08-29T15:25:44.110769Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class VOCDetection(torch.utils.data.Dataset):\n    def __init__(self, root, data):\n        super().__init__()\n        self.root = root\n        self.data = data\n        # self.transforms = transforms\n        # self.bbox = lambda x: [int(x['xmin']), int(x['ymin']), int(x['xmax']), int(x['ymax'])]\n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        file_name = row[0]\n        img = Image.open(os.path.join(self.root, file_name))\n        w, h = img.size\n        bbox = row[1]\n        bboxes = torch.Tensor(bbox)\n        bboxes = tv_tensors.BoundingBoxes(bboxes, format = 'XYXY', canvas_size = (h, w))\n        img, bboxes = resize_transform(img, bboxes, 500)\n        img = transforms.ToTensor()(img)\n        bbox_tensor = bboxes[0]\n        bbox = bbox_tensor.tolist()\n        name = row[2]\n        label = torch.Tensor([name, bbox[0], bbox[1], bbox[2], bbox[3]])\n        return img, label.unsqueeze(0)\n    def __len__(self):\n        return len(self.data)","metadata":{"execution":{"iopub.status.busy":"2024-08-29T15:25:44.113712Z","iopub.execute_input":"2024-08-29T15:25:44.114269Z","iopub.status.idle":"2024-08-29T15:25:44.125819Z","shell.execute_reply.started":"2024-08-29T15:25:44.114226Z","shell.execute_reply":"2024-08-29T15:25:44.124741Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_dataset = VOCDetection(root='/kaggle/working/VOCdevkit/VOC2012/JPEGImages', data=data)","metadata":{"execution":{"iopub.status.busy":"2024-08-29T15:25:44.130823Z","iopub.execute_input":"2024-08-29T15:25:44.131170Z","iopub.status.idle":"2024-08-29T15:25:44.136926Z","shell.execute_reply.started":"2024-08-29T15:25:44.131133Z","shell.execute_reply":"2024-08-29T15:25:44.135567Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-29T15:25:44.138406Z","iopub.execute_input":"2024-08-29T15:25:44.139368Z","iopub.status.idle":"2024-08-29T15:25:44.150816Z","shell.execute_reply.started":"2024-08-29T15:25:44.139319Z","shell.execute_reply":"2024-08-29T15:25:44.149899Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"imgs, labels = next(iter(train_dataloader))\nprint(imgs.shape)\nprint(labels.shape)","metadata":{"execution":{"iopub.status.busy":"2024-08-29T15:25:44.152203Z","iopub.execute_input":"2024-08-29T15:25:44.152962Z","iopub.status.idle":"2024-08-29T15:25:44.337624Z","shell.execute_reply.started":"2024-08-29T15:25:44.152914Z","shell.execute_reply":"2024-08-29T15:25:44.336564Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"torch.Size([8, 3, 500, 500])\ntorch.Size([8, 1, 5])\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/985353478.py:10: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  file_name = row[0]\n/tmp/ipykernel_36/985353478.py:13: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  bbox = row[1]\n/tmp/ipykernel_36/985353478.py:20: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  name = row[2]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"sizes = [[0.2, 0.272], [0.37, 0.447], [0.54, 0.619], [0.71, 0.79], [0.88, 0.961]]\nratios = [[1, 2, 0.5]] * 5\nnum_anchors = len(sizes[0]) + len(ratios[0]) - 1 # n + m - 1\nprint(num_anchors)","metadata":{"execution":{"iopub.status.busy":"2024-08-29T15:25:44.339113Z","iopub.execute_input":"2024-08-29T15:25:44.339746Z","iopub.status.idle":"2024-08-29T15:25:44.347521Z","shell.execute_reply.started":"2024-08-29T15:25:44.339681Z","shell.execute_reply":"2024-08-29T15:25:44.346443Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"4\n","output_type":"stream"}]},{"cell_type":"code","source":"def multibox_prior(data, sizes, ratios):\n    \"\"\"Generate anchor boxes with different shapes centered on each pixel.\"\"\"\n    in_height, in_width = data.shape[-2:]\n    device, num_sizes, num_ratios = data.device, len(sizes), len(ratios)\n    boxes_per_pixel = (num_sizes + num_ratios - 1)\n    size_tensor = torch.tensor(sizes, device=device)\n    ratio_tensor = torch.tensor(ratios, device=device)\n    # Offsets are required to move the anchor to the center of a pixel. Since\n    # a pixel has height=1 and width=1, we choose to offset our centers by 0.5\n    offset_h, offset_w = 0.5, 0.5\n    steps_h = 1.0 / in_height  # Scaled steps in y axis\n    steps_w = 1.0 / in_width  # Scaled steps in x axis\n\n    # Generate all center points for the anchor boxes\n    center_h = (torch.arange(in_height, device=device) + offset_h) * steps_h\n    center_w = (torch.arange(in_width, device=device) + offset_w) * steps_w\n    shift_y, shift_x = torch.meshgrid(center_h, center_w, indexing='ij')\n    shift_y, shift_x = shift_y.reshape(-1), shift_x.reshape(-1)\n\n    # Generate `boxes_per_pixel` number of heights and widths that are later\n    # used to create anchor box corner coordinates (xmin, xmax, ymin, ymax)\n    w = torch.cat((size_tensor * torch.sqrt(ratio_tensor[0]),\n                   sizes[0] * torch.sqrt(ratio_tensor[1:])))\\\n                   * in_height / in_width  # Handle rectangular inputs\n    h = torch.cat((size_tensor / torch.sqrt(ratio_tensor[0]),\n                   sizes[0] / torch.sqrt(ratio_tensor[1:])))\n    # Divide by 2 to get half height and half width\n    anchor_manipulations = torch.stack((-w, -h, w, h)).T.repeat(\n                                        in_height * in_width, 1) / 2\n\n    # Each center point will have `boxes_per_pixel` number of anchor boxes, so\n    # generate a grid of all anchor box centers with `boxes_per_pixel` repeats\n    out_grid = torch.stack([shift_x, shift_y, shift_x, shift_y],\n                dim=1).repeat_interleave(boxes_per_pixel, dim=0)\n    output = out_grid + anchor_manipulations\n    return output.unsqueeze(0)","metadata":{"execution":{"iopub.status.busy":"2024-08-29T15:25:44.349417Z","iopub.execute_input":"2024-08-29T15:25:44.349879Z","iopub.status.idle":"2024-08-29T15:25:44.363549Z","shell.execute_reply.started":"2024-08-29T15:25:44.349830Z","shell.execute_reply":"2024-08-29T15:25:44.362543Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def cls_predictor(num_inputs, num_anchors, num_classes):\n    return nn.Conv2d(num_inputs, num_anchors * (num_classes + 1),\n                     kernel_size=3, padding=1)","metadata":{"execution":{"iopub.status.busy":"2024-08-29T15:25:44.364816Z","iopub.execute_input":"2024-08-29T15:25:44.365427Z","iopub.status.idle":"2024-08-29T15:25:44.374769Z","shell.execute_reply.started":"2024-08-29T15:25:44.365377Z","shell.execute_reply":"2024-08-29T15:25:44.373850Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def bbox_predictor(num_inputs, num_anchors):\n    return nn.Conv2d(num_inputs, num_anchors * 4, kernel_size=3, padding=1)","metadata":{"execution":{"iopub.status.busy":"2024-08-29T15:25:44.375962Z","iopub.execute_input":"2024-08-29T15:25:44.378189Z","iopub.status.idle":"2024-08-29T15:25:44.384910Z","shell.execute_reply.started":"2024-08-29T15:25:44.378151Z","shell.execute_reply":"2024-08-29T15:25:44.383946Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"block1 = nn.Sequential(\n    nn.Conv2d(3, 64, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n    nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n    nn.MaxPool2d(kernel_size=2, stride=2))\n\nblock2 = nn.Sequential(\n    nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n    nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n    nn.MaxPool2d(kernel_size=2, stride=2))\n\nblock3 = nn.Sequential(\n    nn.Conv2d(128, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n    nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n    nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n    nn.MaxPool2d(kernel_size=2, stride=2))\n\nblock4 = nn.Sequential(\n    nn.Conv2d(256, 512, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n    nn.Conv2d(512, 512, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n    nn.Conv2d(512, 512, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n    nn.Conv2d(512, 512, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n    nn.Conv2d(512, 512, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n    nn.Conv2d(512, 512, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n    nn.MaxPool2d(kernel_size=2, stride=2))","metadata":{"execution":{"iopub.status.busy":"2024-08-29T15:25:44.396383Z","iopub.execute_input":"2024-08-29T15:25:44.396735Z","iopub.status.idle":"2024-08-29T15:25:44.552903Z","shell.execute_reply.started":"2024-08-29T15:25:44.396700Z","shell.execute_reply":"2024-08-29T15:25:44.552046Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class SSD_VGG16(nn.Module):\n    def __init__(self, num_classes):\n        super(SSD_VGG16, self).__init__()\n        self.num_classes = num_classes\n        self.block1 = block1\n        self.block2 = block2\n        self.block3 = block3\n        self.block4 = block4\n        self.adaptiveavgpool = nn.AdaptiveAvgPool2d((7,7))\n        self.cls_predictor1 = cls_predictor(64, num_anchors, num_classes)\n        self.bbox_predictor1 = bbox_predictor(64, num_anchors)\n        self.cls_predictor2 = cls_predictor(128, num_anchors, num_classes)\n        self.bbox_predictor2 = bbox_predictor(128, num_anchors)\n        self.cls_predictor3 = cls_predictor(256, num_anchors, num_classes)\n        self.bbox_predictor3 = bbox_predictor(256, num_anchors)\n        self.cls_predictor4 = cls_predictor(512, num_anchors, num_classes)\n        self.bbox_predictor4 = bbox_predictor(512, num_anchors)\n        self.cls_predictor5 = cls_predictor(512, num_anchors, num_classes)\n        self.bbox_predictor5 = bbox_predictor(512, num_anchors)\n\n    def forward(self, x):\n        bs = x.shape[0]\n        out1 = self.block1(x)\n        anchors1 = multibox_prior(out1, sizes[0], ratios[0]).reshape((1, -1))\n        cls_preds1 = self.cls_predictor1(out1).permute(0, 2, 3, 1).reshape(bs, -1)\n        bbox_preds1 = self.bbox_predictor1(out1).permute(0, 2, 3, 1).reshape(bs, -1)\n\n        out2 = self.block2(out1)\n        anchors2 = multibox_prior(out2, sizes[1], ratios[1]).reshape((1, -1))\n        cls_preds2 = self.cls_predictor2(out2).permute(0, 2, 3, 1).reshape(bs, -1)\n        bbox_preds2 = self.bbox_predictor2(out2).permute(0, 2, 3, 1).reshape(bs, -1)\n\n        out3 = self.block3(out2)\n        anchors3 = multibox_prior(out3, sizes[2], ratios[2]).reshape((1, -1))\n        cls_preds3 = self.cls_predictor3(out3).permute(0, 2, 3, 1).reshape(bs, -1)\n        bbox_preds3 = self.bbox_predictor3(out3).permute(0, 2, 3, 1).reshape(bs, -1)\n\n        out4 = self.block4(out3)\n        anchors4 = multibox_prior(out4, sizes[3], ratios[3]).reshape((1, -1))\n        cls_preds4 = self.cls_predictor4(out4).permute(0, 2, 3, 1).reshape(bs, -1)\n        bbox_preds4 = self.bbox_predictor4(out4).permute(0, 2, 3, 1).reshape(bs, -1)\n\n        out5 = self.adaptiveavgpool(out4)\n        anchors5 = multibox_prior(out5, sizes[4], ratios[4]).reshape((1, -1))\n        cls_preds5 = self.cls_predictor5(out5).permute(0, 2, 3, 1).reshape(bs, -1)\n        bbox_preds5 = self.bbox_predictor5(out5).permute(0, 2, 3, 1).reshape(bs, -1)\n\n        anchors = torch.cat([anchors1, anchors2, anchors3, anchors4, anchors5], dim = 1).reshape(1, -1, 4)\n        cls_preds = torch.cat([cls_preds1, cls_preds2, cls_preds3, cls_preds4, cls_preds5], dim = 1).reshape(bs, -1, self.num_classes+1)\n        bbox_preds = torch.cat([bbox_preds1, bbox_preds2, bbox_preds3, bbox_preds4, bbox_preds5], dim = 1).reshape(bs, -1)\n        return anchors, cls_preds, bbox_preds","metadata":{"execution":{"iopub.status.busy":"2024-08-29T15:25:44.554273Z","iopub.execute_input":"2024-08-29T15:25:44.554672Z","iopub.status.idle":"2024-08-29T15:25:44.576829Z","shell.execute_reply.started":"2024-08-29T15:25:44.554617Z","shell.execute_reply":"2024-08-29T15:25:44.575748Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"net = SSD_VGG16(num_classes=20)\nX = torch.zeros((8, 3, 500, 500))\nanchors, cls_preds, bbox_preds = net(X)\n\nprint('output anchors:', anchors.shape)\nprint('output class preds:', cls_preds.shape)\nprint('output bbox preds:', bbox_preds.shape)","metadata":{"execution":{"iopub.status.busy":"2024-08-29T15:25:44.578664Z","iopub.execute_input":"2024-08-29T15:25:44.579032Z","iopub.status.idle":"2024-08-29T15:25:56.260228Z","shell.execute_reply.started":"2024-08-29T15:25:44.578995Z","shell.execute_reply":"2024-08-29T15:25:56.259104Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"output anchors: torch.Size([1, 328972, 4])\noutput class preds: torch.Size([8, 328972, 21])\noutput bbox preds: torch.Size([8, 1315888])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\n!pip install torchsummary\nfrom torchsummary import summary\nsummary(net.to(device), (3, 500, 500))","metadata":{"execution":{"iopub.status.busy":"2024-08-29T15:25:56.261756Z","iopub.execute_input":"2024-08-29T15:25:56.262364Z","iopub.status.idle":"2024-08-29T15:26:14.646797Z","shell.execute_reply.started":"2024-08-29T15:25:56.262310Z","shell.execute_reply":"2024-08-29T15:26:14.645457Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\nDownloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 500, 500]           1,792\n              ReLU-2         [-1, 64, 500, 500]               0\n            Conv2d-3         [-1, 64, 500, 500]          36,928\n              ReLU-4         [-1, 64, 500, 500]               0\n         MaxPool2d-5         [-1, 64, 250, 250]               0\n            Conv2d-6         [-1, 84, 250, 250]          48,468\n            Conv2d-7         [-1, 16, 250, 250]           9,232\n            Conv2d-8        [-1, 128, 250, 250]          73,856\n              ReLU-9        [-1, 128, 250, 250]               0\n           Conv2d-10        [-1, 128, 250, 250]         147,584\n             ReLU-11        [-1, 128, 250, 250]               0\n        MaxPool2d-12        [-1, 128, 125, 125]               0\n           Conv2d-13         [-1, 84, 125, 125]          96,852\n           Conv2d-14         [-1, 16, 125, 125]          18,448\n           Conv2d-15        [-1, 256, 125, 125]         295,168\n             ReLU-16        [-1, 256, 125, 125]               0\n           Conv2d-17        [-1, 256, 125, 125]         590,080\n             ReLU-18        [-1, 256, 125, 125]               0\n           Conv2d-19        [-1, 256, 125, 125]         590,080\n             ReLU-20        [-1, 256, 125, 125]               0\n        MaxPool2d-21          [-1, 256, 62, 62]               0\n           Conv2d-22           [-1, 84, 62, 62]         193,620\n           Conv2d-23           [-1, 16, 62, 62]          36,880\n           Conv2d-24          [-1, 512, 62, 62]       1,180,160\n             ReLU-25          [-1, 512, 62, 62]               0\n           Conv2d-26          [-1, 512, 62, 62]       2,359,808\n             ReLU-27          [-1, 512, 62, 62]               0\n           Conv2d-28          [-1, 512, 62, 62]       2,359,808\n             ReLU-29          [-1, 512, 62, 62]               0\n        MaxPool2d-30          [-1, 512, 31, 31]               0\n           Conv2d-31          [-1, 512, 31, 31]       2,359,808\n             ReLU-32          [-1, 512, 31, 31]               0\n           Conv2d-33          [-1, 512, 31, 31]       2,359,808\n             ReLU-34          [-1, 512, 31, 31]               0\n           Conv2d-35          [-1, 512, 31, 31]       2,359,808\n             ReLU-36          [-1, 512, 31, 31]               0\n        MaxPool2d-37          [-1, 512, 15, 15]               0\n           Conv2d-38           [-1, 84, 15, 15]         387,156\n           Conv2d-39           [-1, 16, 15, 15]          73,744\nAdaptiveAvgPool2d-40            [-1, 512, 7, 7]               0\n           Conv2d-41             [-1, 84, 7, 7]         387,156\n           Conv2d-42             [-1, 16, 7, 7]          73,744\n================================================================\nTotal params: 16,039,988\nTrainable params: 16,039,988\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 2.86\nForward/backward pass size (MB): 1149.00\nParams size (MB): 61.19\nEstimated Total Size (MB): 1213.05\n----------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"def box_iou(boxes1, boxes2):\n    \"\"\"Compute pairwise IoU across two lists of anchor or bounding boxes.\"\"\"\n    box_area = lambda boxes: ((boxes[:, 2] - boxes[:, 0]) *\n                              (boxes[:, 3] - boxes[:, 1]))\n    # Shape of `boxes1`, `boxes2`, `areas1`, `areas2`: (no. of boxes1, 4),\n    # (no. of boxes2, 4), (no. of boxes1,), (no. of boxes2,)\n    areas1 = box_area(boxes1)\n    areas2 = box_area(boxes2)\n    # Shape of `inter_upperlefts`, `inter_lowerrights`, `inters`: (no. of\n    # boxes1, no. of boxes2, 2)\n    inter_upperlefts = torch.max(boxes1[:, None, :2], boxes2[:, :2])\n    inter_lowerrights = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])\n    inters = (inter_lowerrights - inter_upperlefts).clamp(min=0)\n    # Shape of `inter_areas` and `union_areas`: (no. of boxes1, no. of boxes2)\n    inter_areas = inters[:, :, 0] * inters[:, :, 1]\n    union_areas = areas1[:, None] + areas2 - inter_areas\n    return inter_areas / union_areas","metadata":{"execution":{"iopub.status.busy":"2024-08-29T15:26:14.648633Z","iopub.execute_input":"2024-08-29T15:26:14.649029Z","iopub.status.idle":"2024-08-29T15:26:14.658814Z","shell.execute_reply.started":"2024-08-29T15:26:14.648988Z","shell.execute_reply":"2024-08-29T15:26:14.657569Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def assign_anchor_to_bbox(ground_truth, anchors, device, iou_threshold=0.5):\n    \"\"\"Assign closest ground-truth bounding boxes to anchor boxes.\"\"\"\n    num_anchors, num_gt_boxes = anchors.shape[0], ground_truth.shape[0]\n    # Element x_ij in the i-th row and j-th column is the IoU of the anchor\n    # box i and the ground-truth bounding box j\n    jaccard = box_iou(anchors, ground_truth)\n    # Initialize the tensor to hold the assigned ground-truth bounding box for\n    # each anchor\n    anchors_bbox_map = torch.full((num_anchors,), -1, dtype=torch.long,\n                                  device=device)\n    # Assign ground-truth bounding boxes according to the threshold\n    max_ious, indices = torch.max(jaccard, dim=1)\n    anc_i = torch.nonzero(max_ious >= iou_threshold).reshape(-1)\n    box_j = indices[max_ious >= iou_threshold]\n    anchors_bbox_map[anc_i] = box_j\n    col_discard = torch.full((num_anchors,), -1)\n    row_discard = torch.full((num_gt_boxes,), -1)\n    for _ in range(num_gt_boxes):\n        max_idx = torch.argmax(jaccard)  # Find the largest IoU\n        box_idx = (max_idx % num_gt_boxes).long()\n        anc_idx = (max_idx / num_gt_boxes).long()\n        anchors_bbox_map[anc_idx] = box_idx\n        jaccard[:, box_idx] = col_discard\n        jaccard[anc_idx, :] = row_discard\n    return anchors_bbox_map","metadata":{"execution":{"iopub.status.busy":"2024-08-29T15:26:14.660710Z","iopub.execute_input":"2024-08-29T15:26:14.661695Z","iopub.status.idle":"2024-08-29T15:26:14.675712Z","shell.execute_reply.started":"2024-08-29T15:26:14.661646Z","shell.execute_reply":"2024-08-29T15:26:14.674638Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def box_corner_to_center(boxes):\n    \"\"\" Convert from (upper-left, lower-right) to (center, width, height).\"\"\"\n    x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:,3]\n    cx = (x1 + x2) / 2\n    cy = (y1 + y2) / 2\n    w = x2 - x1\n    h = y2 - y1\n    boxes = torch.stack((cx, cy, w, h), axis=-1)\n    return boxes","metadata":{"execution":{"iopub.status.busy":"2024-08-29T15:26:14.677217Z","iopub.execute_input":"2024-08-29T15:26:14.677613Z","iopub.status.idle":"2024-08-29T15:26:14.690777Z","shell.execute_reply.started":"2024-08-29T15:26:14.677562Z","shell.execute_reply":"2024-08-29T15:26:14.689810Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def offset_boxes(anchors, assigned_bb, eps=1e-6):\n    \"\"\"Transform for anchor box offsets.\"\"\"\n    c_anc = box_corner_to_center(anchors)\n    c_assigned_bb = box_corner_to_center(assigned_bb)\n    offset_xy = 10 * (c_assigned_bb[:, :2] - c_anc[:, :2]) / c_anc[:, 2:]\n    offset_wh = 5 * torch.log(eps + c_assigned_bb[:, 2:] / c_anc[:, 2:])\n    offset = torch.cat([offset_xy, offset_wh], axis=1)\n    return offset","metadata":{"execution":{"iopub.status.busy":"2024-08-29T15:26:14.692208Z","iopub.execute_input":"2024-08-29T15:26:14.692536Z","iopub.status.idle":"2024-08-29T15:26:14.702335Z","shell.execute_reply.started":"2024-08-29T15:26:14.692500Z","shell.execute_reply":"2024-08-29T15:26:14.701321Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def multibox_target(anchors, labels):\n    \"\"\"Label anchor boxes using ground-truth bounding boxes.\"\"\"\n    batch_size, anchors = labels.shape[0], anchors.squeeze(0)\n    batch_offset, batch_mask, batch_class_labels = [], [], []\n    device, num_anchors = anchors.device, anchors.shape[0]\n    for i in range(batch_size):\n        label = labels[i, :, :]\n        anchors_bbox_map = assign_anchor_to_bbox(\n            label[:, 1:], anchors, device)\n        bbox_mask = ((anchors_bbox_map >= 0).float().unsqueeze(-1)).repeat(1, 4)\n        # Initialize class labels and assigned bounding box coordinates with\n        # zeros\n        class_labels = torch.zeros(num_anchors, dtype=torch.long,\n                                   device=device)\n        assigned_bb = torch.zeros((num_anchors, 4), dtype=torch.float32,\n                                  device=device)\n        # Label classes of anchor boxes using their assigned ground-truth\n        # bounding boxes. If an anchor box is not assigned any, we label its\n        # class as background (the value remains zero)\n        indices_true = torch.nonzero(anchors_bbox_map >= 0)\n        bb_idx = anchors_bbox_map[indices_true]\n        class_labels[indices_true] = label[bb_idx, 0].long() + 1\n        assigned_bb[indices_true] = label[bb_idx, 1:]\n        # Offset transformation\n        offset = offset_boxes(anchors, assigned_bb) * bbox_mask\n        batch_offset.append(offset.reshape(-1))\n        batch_mask.append(bbox_mask.reshape(-1))\n        batch_class_labels.append(class_labels)\n    bbox_offset = torch.stack(batch_offset)\n    bbox_mask = torch.stack(batch_mask)\n    class_labels = torch.stack(batch_class_labels)\n    return (bbox_offset, bbox_mask, class_labels)","metadata":{"execution":{"iopub.status.busy":"2024-08-29T15:26:14.703922Z","iopub.execute_input":"2024-08-29T15:26:14.704454Z","iopub.status.idle":"2024-08-29T15:26:14.718575Z","shell.execute_reply.started":"2024-08-29T15:26:14.704403Z","shell.execute_reply":"2024-08-29T15:26:14.717599Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"trainer = torch.optim.NAdam(net.parameters(), lr=0.0001, weight_decay=5e-4)","metadata":{"execution":{"iopub.status.busy":"2024-08-29T15:26:14.719987Z","iopub.execute_input":"2024-08-29T15:26:14.720334Z","iopub.status.idle":"2024-08-29T15:26:14.733829Z","shell.execute_reply.started":"2024-08-29T15:26:14.720299Z","shell.execute_reply":"2024-08-29T15:26:14.732813Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"cls_loss = nn.CrossEntropyLoss(reduction='sum')\nbbox_loss = nn.L1Loss(reduction='sum')\n\ndef calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks):\n    batch_size, num_classes = cls_preds.shape[0], cls_preds.shape[2]\n    # bbox_loss\n    bbox = bbox_loss(bbox_preds * bbox_masks,\n                     bbox_labels * bbox_masks)\n    cls_mask = bbox_masks.reshape(batch_size, -1, 4)\n    cls_mask = cls_mask.sum(dim = -1).reshape(-1)\n    foreground_idxs = torch.where(cls_mask > 0)[0]\n    num_foreground = foreground_idxs.shape[0]\n\n    background_idxs = torch.where(cls_mask == 0)[0]\n    keep_bg_idxs = background_idxs[torch.rand(*background_idxs.shape)>0.95]\n    cls_mask[keep_bg_idxs] = 4\n    cls = cls_loss(cls_preds.reshape(-1, num_classes)[cls_mask>0],\n                   cls_labels.reshape(-1)[cls_mask>0])\n    return cls/num_foreground, bbox/num_foreground","metadata":{"execution":{"iopub.status.busy":"2024-08-29T15:26:14.740953Z","iopub.execute_input":"2024-08-29T15:26:14.741767Z","iopub.status.idle":"2024-08-29T15:26:14.752451Z","shell.execute_reply.started":"2024-08-29T15:26:14.741723Z","shell.execute_reply":"2024-08-29T15:26:14.751370Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def cls_eval(cls_preds, cls_labels):\n    # Because the class prediction results are on the final dimension,\n    # `argmax` needs to specify this dimension\n    return float((cls_preds.argmax(dim=-1).type(\n        cls_labels.dtype) == cls_labels).sum())\n\ndef bbox_eval(bbox_preds, bbox_labels, bbox_masks):\n    return float((torch.abs((bbox_labels - bbox_preds) * bbox_masks)).sum())","metadata":{"execution":{"iopub.status.busy":"2024-08-29T15:26:14.753777Z","iopub.execute_input":"2024-08-29T15:26:14.754199Z","iopub.status.idle":"2024-08-29T15:26:14.764511Z","shell.execute_reply.started":"2024-08-29T15:26:14.754142Z","shell.execute_reply":"2024-08-29T15:26:14.763425Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"num_epochs, timer = 10, Timer()\nnet = net.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-29T15:26:14.765822Z","iopub.execute_input":"2024-08-29T15:26:14.766186Z","iopub.status.idle":"2024-08-29T15:26:14.775161Z","shell.execute_reply.started":"2024-08-29T15:26:14.766137Z","shell.execute_reply":"2024-08-29T15:26:14.774118Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"metric = {'train_loss': [],\n          'cls_loss': [],\n          'bbox_loss': []}\nfor epoch in range(num_epochs):\n    # Sum of training accuracy, no. of examples in sum of training accuracy,\n    # Sum of absolute error, no. of examples in sum of absolute error\n    net.train()\n    for features, target in train_dataloader:\n        timer.start()\n        trainer.zero_grad()\n        X, Y = features.to(device), target.to(device)\n        # Generate multiscale anchor boxes and predict their classes and\n        # offsets\n        anchors, cls_preds, bbox_preds = net(X)\n        # Label the classes and offsets of these anchor boxes\n        bbox_labels, bbox_masks, cls_labels = multibox_target(anchors, Y)\n        # Calculate the loss function using the predicted and labeled values\n        # of the classes and offsets\n        cls_l, bbox_l = calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels,\n                      bbox_masks)\n        l = cls_l + bbox_l\n        l.backward()\n        trainer.step()\n        metric['train_loss'].append(l.item())\n        metric['cls_loss'].append(cls_l.item())\n        metric['bbox_loss'].append(bbox_l.item())\n        # print('Loss: %.2f Cls_loss: %.2f Bbox_loss %.2f' %(l.item(), cls_l.item(), bbox_l.item()))\n    print(f'Epoch {epoch}: Loss: %.2f Cls_loss: %.2f Bbox_loss %.2f'%(sum(metric['train_loss'][-len(train_dataloader):])/len(train_dataloader),\n                                                                      sum(metric['cls_loss'][-len(train_dataloader):])/len(train_dataloader),\n                                                                      sum(metric['bbox_loss'][-len(train_dataloader):])/len(train_dataloader)))","metadata":{"execution":{"iopub.status.busy":"2024-08-29T15:26:14.776358Z","iopub.execute_input":"2024-08-29T15:26:14.776688Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/985353478.py:10: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  file_name = row[0]\n/tmp/ipykernel_36/985353478.py:13: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  bbox = row[1]\n/tmp/ipykernel_36/985353478.py:20: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  name = row[2]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0: Loss: 26749.21 Cls_loss: 1130.34 Bbox_loss 25618.86\nEpoch 1: Loss: 25303.15 Cls_loss: 2.84 Bbox_loss 25300.31\nEpoch 2: Loss: 20611.57 Cls_loss: 2.74 Bbox_loss 20608.83\nEpoch 3: Loss: 8954.15 Cls_loss: 2.92 Bbox_loss 8951.24\nEpoch 4: Loss: 7653.20 Cls_loss: 3.14 Bbox_loss 7650.07\n","output_type":"stream"}]}]}