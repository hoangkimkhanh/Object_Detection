{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import libraries","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport time\nfrom torchvision import tv_tensors\nfrom torchvision.transforms import v2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-19T11:16:40.116890Z","iopub.execute_input":"2024-09-19T11:16:40.117232Z","iopub.status.idle":"2024-09-19T11:16:44.955314Z","shell.execute_reply.started":"2024-09-19T11:16:40.117196Z","shell.execute_reply":"2024-09-19T11:16:44.954409Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"class Timer:\n    \"\"\"Record multiple running times.\"\"\"\n    def __init__(self):\n        \"\"\"Defined in :numref:`sec_minibatch_sgd`\"\"\"\n        self.times = []\n        self.start()\n\n    def start(self):\n        \"\"\"Start the timer.\"\"\"\n        self.tik = time.time()\n\n    def stop(self):\n        \"\"\"Stop the timer and record the time in a list.\"\"\"\n        self.times.append(time.time() - self.tik)\n        return self.times[-1]\n\n    def avg(self):\n        \"\"\"Return the average time.\"\"\"\n        return sum(self.times) / len(self.times)\n\n    def sum(self):\n        \"\"\"Return the sum of time.\"\"\"\n        return sum(self.times)\n\n    def cumsum(self):\n        \"\"\"Return the accumulated time.\"\"\"\n        return np.array(self.times).cumsum().tolist()","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:16:44.957041Z","iopub.execute_input":"2024-09-19T11:16:44.957415Z","iopub.status.idle":"2024-09-19T11:16:44.965759Z","shell.execute_reply.started":"2024-09-19T11:16:44.957382Z","shell.execute_reply":"2024-09-19T11:16:44.963841Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"voc_trainset = torchvision.datasets.VOCDetection(\n    root='/kaggle/working/', year='2012', image_set='train', download=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:16:44.967349Z","iopub.execute_input":"2024-09-19T11:16:44.967765Z","iopub.status.idle":"2024-09-19T11:17:03.897676Z","shell.execute_reply.started":"2024-09-19T11:16:44.967703Z","shell.execute_reply":"2024-09-19T11:17:03.896685Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar to /kaggle/working/VOCtrainval_11-May-2012.tar\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1999639040/1999639040 [00:06<00:00, 296323674.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting /kaggle/working/VOCtrainval_11-May-2012.tar to /kaggle/working/\n","output_type":"stream"}]},{"cell_type":"code","source":"voc_valset = torchvision.datasets.VOCDetection(\n    root='/kaggle/working/', year='2012', image_set='val', download=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:17:03.900071Z","iopub.execute_input":"2024-09-19T11:17:03.900388Z","iopub.status.idle":"2024-09-19T11:17:18.166335Z","shell.execute_reply.started":"2024-09-19T11:17:03.900355Z","shell.execute_reply":"2024-09-19T11:17:18.165510Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Using downloaded and verified file: /kaggle/working/VOCtrainval_11-May-2012.tar\nExtracting /kaggle/working/VOCtrainval_11-May-2012.tar to /kaggle/working/\n","output_type":"stream"}]},{"cell_type":"code","source":"VOC_I2N = {0:'aeroplane',\n1:'bicycle',\n2:'bird',\n3:'boat',\n4:'bottle',\n5:'bus',\n6:'car',\n7:'cat',\n8:'chair',\n9:'cow',\n10:'diningtable',\n11:'dog',\n12:'horse',\n13:'motorbike',\n14:'person',\n15:'pottedplant',\n16:'sheep',\n17:'sofa',\n18:'train',\n19:'tvmonitor'}\nVOC_N2I = {n: i for (i, n) in zip(VOC_I2N.keys(), VOC_I2N.values())}","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:17:18.167350Z","iopub.execute_input":"2024-09-19T11:17:18.167635Z","iopub.status.idle":"2024-09-19T11:17:18.173906Z","shell.execute_reply.started":"2024-09-19T11:17:18.167605Z","shell.execute_reply":"2024-09-19T11:17:18.173012Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nbbox = lambda x: [int(x['xmin']), int(x['ymin']), int(x['xmax']), int(x['ymax'])]\ndic = {'file_name':[], 'bbox': [], 'name': []}\nfor img, annotation in tqdm(voc_trainset):\n    for obj in annotation['annotation']['object']:\n        dic['file_name'].append(annotation['annotation']['filename'])\n        dic['bbox'].append(bbox(obj['bndbox']))\n        dic['name'].append(VOC_N2I[obj['name']])","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:17:18.175297Z","iopub.execute_input":"2024-09-19T11:17:18.175598Z","iopub.status.idle":"2024-09-19T11:17:35.370431Z","shell.execute_reply.started":"2024-09-19T11:17:18.175566Z","shell.execute_reply":"2024-09-19T11:17:35.369534Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"100%|██████████| 5717/5717 [00:16<00:00, 348.76it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\ndata = pd.DataFrame(dic)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:17:35.371859Z","iopub.execute_input":"2024-09-19T11:17:35.372250Z","iopub.status.idle":"2024-09-19T11:17:35.827171Z","shell.execute_reply.started":"2024-09-19T11:17:35.372205Z","shell.execute_reply":"2024-09-19T11:17:35.826305Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"         file_name                 bbox  name\n0  2008_000008.jpg   [53, 87, 471, 420]    12\n1  2008_000008.jpg  [158, 44, 289, 167]    14\n2  2008_000015.jpg   [270, 1, 378, 176]     4\n3  2008_000015.jpg    [57, 1, 164, 150]     4\n4  2008_000019.jpg   [139, 2, 372, 197]    11","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_name</th>\n      <th>bbox</th>\n      <th>name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2008_000008.jpg</td>\n      <td>[53, 87, 471, 420]</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2008_000008.jpg</td>\n      <td>[158, 44, 289, 167]</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2008_000015.jpg</td>\n      <td>[270, 1, 378, 176]</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2008_000015.jpg</td>\n      <td>[57, 1, 164, 150]</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2008_000019.jpg</td>\n      <td>[139, 2, 372, 197]</td>\n      <td>11</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def resize_transform(image, target, max_size):\n    w, h = image.size\n    w_delta = w - max_size\n    h_delta = h - max_size\n    if w_delta > 0:\n        image, target = v2.Pad([w_delta//2, 0])(image, target)\n        w = max_size\n    else:\n        image, target = v2.CenterCrop([h, max_size])(image, target)\n    if h_delta > 0:\n        image, target = v2.Pad([0, h_delta//2])(image, target)\n        h = max_size\n    else:\n        image, target = v2.CenterCrop([max_size, h])(image, target)\n    image, target = v2.Resize((max_size, max_size))(image, target)\n    return image, target","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:17:35.828276Z","iopub.execute_input":"2024-09-19T11:17:35.828687Z","iopub.status.idle":"2024-09-19T11:17:35.835914Z","shell.execute_reply.started":"2024-09-19T11:17:35.828656Z","shell.execute_reply":"2024-09-19T11:17:35.835009Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class VOCDetection(torch.utils.data.Dataset):\n    def __init__(self, root, data):\n        super().__init__()\n        self.root = root\n        self.data = data\n        # self.transforms = transforms\n        # self.bbox = lambda x: [int(x['xmin']), int(x['ymin']), int(x['xmax']), int(x['ymax'])]\n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        file_name = row[0]\n        img = Image.open(os.path.join(self.root, file_name))\n        w, h = img.size\n        bbox = row[1]\n        bboxes = torch.Tensor(bbox)\n        bboxes = tv_tensors.BoundingBoxes(bboxes, format = 'XYXY', canvas_size = (h, w))\n        img, bboxes = resize_transform(img, bboxes, 500)\n        img = transforms.ToTensor()(img)\n        bbox_tensor = bboxes[0]\n        bbox = bbox_tensor.tolist()\n        name = row[2]\n        label = torch.Tensor([name, bbox[0], bbox[1], bbox[2], bbox[3]])\n        return img, label.unsqueeze(0)\n    def __len__(self):\n        return len(self.data)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:17:35.836983Z","iopub.execute_input":"2024-09-19T11:17:35.837264Z","iopub.status.idle":"2024-09-19T11:17:35.847662Z","shell.execute_reply.started":"2024-09-19T11:17:35.837233Z","shell.execute_reply":"2024-09-19T11:17:35.846772Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_dataset = VOCDetection(root='/kaggle/working/VOCdevkit/VOC2012/JPEGImages', data=data)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:17:35.852031Z","iopub.execute_input":"2024-09-19T11:17:35.852317Z","iopub.status.idle":"2024-09-19T11:17:35.857314Z","shell.execute_reply.started":"2024-09-19T11:17:35.852287Z","shell.execute_reply":"2024-09-19T11:17:35.856518Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:17:35.858499Z","iopub.execute_input":"2024-09-19T11:17:35.858908Z","iopub.status.idle":"2024-09-19T11:17:35.869657Z","shell.execute_reply.started":"2024-09-19T11:17:35.858867Z","shell.execute_reply":"2024-09-19T11:17:35.868877Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"imgs, labels = next(iter(train_dataloader))\nprint(imgs.shape)\nprint(labels.shape)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:17:35.870707Z","iopub.execute_input":"2024-09-19T11:17:35.871016Z","iopub.status.idle":"2024-09-19T11:17:36.030167Z","shell.execute_reply.started":"2024-09-19T11:17:35.870986Z","shell.execute_reply":"2024-09-19T11:17:36.029221Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"torch.Size([8, 3, 500, 500])\ntorch.Size([8, 1, 5])\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/985353478.py:10: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  file_name = row[0]\n/tmp/ipykernel_36/985353478.py:13: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  bbox = row[1]\n/tmp/ipykernel_36/985353478.py:20: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  name = row[2]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"sizes = [[0.2, 0.272], [0.37, 0.447], [0.54, 0.619], [0.71, 0.79], [0.88, 0.961]]\nratios = [[1, 2, 0.5]] * 5\nnum_anchors = len(sizes[0]) + len(ratios[0]) - 1 # n + m - 1\nprint(num_anchors)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:17:36.031462Z","iopub.execute_input":"2024-09-19T11:17:36.031791Z","iopub.status.idle":"2024-09-19T11:17:36.038172Z","shell.execute_reply.started":"2024-09-19T11:17:36.031755Z","shell.execute_reply":"2024-09-19T11:17:36.037257Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"4\n","output_type":"stream"}]},{"cell_type":"code","source":"def multibox_prior(data, sizes, ratios):\n    \"\"\"Generate anchor boxes with different shapes centered on each pixel.\"\"\"\n    in_height, in_width = data.shape[-2:]\n    device, num_sizes, num_ratios = data.device, len(sizes), len(ratios)\n    boxes_per_pixel = (num_sizes + num_ratios - 1)\n    size_tensor = torch.tensor(sizes, device=device)\n    ratio_tensor = torch.tensor(ratios, device=device)\n    # Offsets are required to move the anchor to the center of a pixel. Since\n    # a pixel has height=1 and width=1, we choose to offset our centers by 0.5\n    offset_h, offset_w = 0.5, 0.5\n    steps_h = 1.0 / in_height  # Scaled steps in y axis\n    steps_w = 1.0 / in_width  # Scaled steps in x axis\n\n    # Generate all center points for the anchor boxes\n    center_h = (torch.arange(in_height, device=device) + offset_h) * steps_h\n    center_w = (torch.arange(in_width, device=device) + offset_w) * steps_w\n    shift_y, shift_x = torch.meshgrid(center_h, center_w, indexing='ij')\n    shift_y, shift_x = shift_y.reshape(-1), shift_x.reshape(-1)\n\n    # Generate `boxes_per_pixel` number of heights and widths that are later\n    # used to create anchor box corner coordinates (xmin, xmax, ymin, ymax)\n    w = torch.cat((size_tensor * torch.sqrt(ratio_tensor[0]),\n                   sizes[0] * torch.sqrt(ratio_tensor[1:])))\\\n                   * in_height / in_width  # Handle rectangular inputs\n    h = torch.cat((size_tensor / torch.sqrt(ratio_tensor[0]),\n                   sizes[0] / torch.sqrt(ratio_tensor[1:])))\n    # Divide by 2 to get half height and half width\n    anchor_manipulations = torch.stack((-w, -h, w, h)).T.repeat(\n                                        in_height * in_width, 1) / 2\n\n    # Each center point will have `boxes_per_pixel` number of anchor boxes, so\n    # generate a grid of all anchor box centers with `boxes_per_pixel` repeats\n    out_grid = torch.stack([shift_x, shift_y, shift_x, shift_y],\n                dim=1).repeat_interleave(boxes_per_pixel, dim=0)\n    output = out_grid + anchor_manipulations\n    return output.unsqueeze(0)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:17:36.039539Z","iopub.execute_input":"2024-09-19T11:17:36.039891Z","iopub.status.idle":"2024-09-19T11:17:36.051142Z","shell.execute_reply.started":"2024-09-19T11:17:36.039844Z","shell.execute_reply":"2024-09-19T11:17:36.050199Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def cls_predictor(num_inputs, num_anchors, num_classes):\n    return nn.Conv2d(num_inputs, num_anchors * (num_classes + 1),\n                     kernel_size=3, padding=1)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:17:36.052334Z","iopub.execute_input":"2024-09-19T11:17:36.052739Z","iopub.status.idle":"2024-09-19T11:17:36.062798Z","shell.execute_reply.started":"2024-09-19T11:17:36.052664Z","shell.execute_reply":"2024-09-19T11:17:36.061930Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def bbox_predictor(num_inputs, num_anchors):\n    return nn.Conv2d(num_inputs, num_anchors * 4, kernel_size=3, padding=1)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:17:36.064359Z","iopub.execute_input":"2024-09-19T11:17:36.064767Z","iopub.status.idle":"2024-09-19T11:17:36.071332Z","shell.execute_reply.started":"2024-09-19T11:17:36.064729Z","shell.execute_reply":"2024-09-19T11:17:36.070395Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class SSD_ResNet18(nn.Module):\n    def __init__(self, num_classes):\n        super(SSD_ResNet18, self).__init__()\n        self.num_classes = num_classes\n        self.resnet18 = torchvision.models.resnet18(pretrained=True)\n        self.adaptivemaxpool = nn.AdaptiveMaxPool2d((1,1))\n        self.cls_predictor1 = cls_predictor(64, num_anchors, num_classes)\n        self.bbox_predictor1 = bbox_predictor(64, num_anchors)\n        self.cls_predictor2 = cls_predictor(128, num_anchors, num_classes)\n        self.bbox_predictor2 = bbox_predictor(128, num_anchors)\n        self.cls_predictor3 = cls_predictor(256, num_anchors, num_classes)\n        self.bbox_predictor3 = bbox_predictor(256, num_anchors)\n        self.cls_predictor4 = cls_predictor(512, num_anchors, num_classes)\n        self.bbox_predictor4 = bbox_predictor(512, num_anchors)\n        self.cls_predictor5 = cls_predictor(512, num_anchors, num_classes)\n        self.bbox_predictor5 = bbox_predictor(512, num_anchors)\n\n    def forward(self, x):\n        bs = x.shape[0]\n        out1 = self.resnet18.layer1(self.resnet18.maxpool(self.resnet18.relu(self.resnet18.bn1(self.resnet18.conv1(x)))))\n        anchors1 = multibox_prior(out1, sizes[0], ratios[0]).reshape((1, -1))\n        cls_preds1 = self.cls_predictor1(out1).permute(0, 2, 3, 1).reshape(bs, -1)\n        bbox_preds1 = self.bbox_predictor1(out1).permute(0, 2, 3, 1).reshape(bs, -1)\n\n        out2 = self.resnet18.layer2(out1)\n        anchors2 = multibox_prior(out2, sizes[1], ratios[1]).reshape((1, -1))\n        cls_preds2 = self.cls_predictor2(out2).permute(0, 2, 3, 1).reshape(bs, -1)\n        bbox_preds2 = self.bbox_predictor2(out2).permute(0, 2, 3, 1).reshape(bs, -1)\n\n        out3 = self.resnet18.layer3(out2)\n        anchors3 = multibox_prior(out3, sizes[2], ratios[2]).reshape((1, -1))\n        cls_preds3 = self.cls_predictor3(out3).permute(0, 2, 3, 1).reshape(bs, -1)\n        bbox_preds3 = self.bbox_predictor3(out3).permute(0, 2, 3, 1).reshape(bs, -1)\n\n        out4 = self.resnet18.layer4(out3)\n        anchors4 = multibox_prior(out4, sizes[3], ratios[3]).reshape((1, -1))\n        cls_preds4 = self.cls_predictor4(out4).permute(0, 2, 3, 1).reshape(bs, -1)\n        bbox_preds4 = self.bbox_predictor4(out4).permute(0, 2, 3, 1).reshape(bs, -1)\n\n        out5 = self.adaptivemaxpool(out4)\n        anchors5 = multibox_prior(out5, sizes[4], ratios[4]).reshape((1, -1))\n        cls_preds5 = self.cls_predictor5(out5).permute(0, 2, 3, 1).reshape(bs, -1)\n        bbox_preds5 = self.bbox_predictor5(out5).permute(0, 2, 3, 1).reshape(bs, -1)\n\n        anchors = torch.cat([anchors1, anchors2, anchors3, anchors4, anchors5], dim = 1).reshape(1, -1, 4)\n        cls_preds = torch.cat([cls_preds1, cls_preds2, cls_preds3, cls_preds4, cls_preds5], dim = 1).reshape(bs, -1, self.num_classes+1)\n        bbox_preds = torch.cat([bbox_preds1, bbox_preds2, bbox_preds3, bbox_preds4, bbox_preds5], dim = 1).reshape(bs, -1)\n        return anchors, cls_preds, bbox_preds","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:17:36.072822Z","iopub.execute_input":"2024-09-19T11:17:36.073356Z","iopub.status.idle":"2024-09-19T11:17:36.093243Z","shell.execute_reply.started":"2024-09-19T11:17:36.073312Z","shell.execute_reply":"2024-09-19T11:17:36.092273Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"net = SSD_ResNet18(num_classes=20)\nX = torch.zeros((8, 3, 500, 500))\nanchors, cls_preds, bbox_preds = net(X)\n\nprint('output anchors:', anchors.shape)\nprint('output class preds:', cls_preds.shape)\nprint('output bbox preds:', bbox_preds.shape)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:17:36.094262Z","iopub.execute_input":"2024-09-19T11:17:36.094624Z","iopub.status.idle":"2024-09-19T11:17:38.268263Z","shell.execute_reply.started":"2024-09-19T11:17:36.094582Z","shell.execute_reply":"2024-09-19T11:17:38.267088Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 206MB/s]\n","output_type":"stream"},{"name":"stdout","text":"output anchors: torch.Size([1, 83500, 4])\noutput class preds: torch.Size([8, 83500, 21])\noutput bbox preds: torch.Size([8, 334000])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\n!pip install torchsummary\nfrom torchsummary import summary\nsummary(net.to(device), (3, 500, 500))","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:17:38.269486Z","iopub.execute_input":"2024-09-19T11:17:38.269889Z","iopub.status.idle":"2024-09-19T11:17:53.002526Z","shell.execute_reply.started":"2024-09-19T11:17:38.269846Z","shell.execute_reply":"2024-09-19T11:17:53.001435Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\nDownloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 250, 250]           9,408\n       BatchNorm2d-2         [-1, 64, 250, 250]             128\n              ReLU-3         [-1, 64, 250, 250]               0\n         MaxPool2d-4         [-1, 64, 125, 125]               0\n            Conv2d-5         [-1, 64, 125, 125]          36,864\n       BatchNorm2d-6         [-1, 64, 125, 125]             128\n              ReLU-7         [-1, 64, 125, 125]               0\n            Conv2d-8         [-1, 64, 125, 125]          36,864\n       BatchNorm2d-9         [-1, 64, 125, 125]             128\n             ReLU-10         [-1, 64, 125, 125]               0\n       BasicBlock-11         [-1, 64, 125, 125]               0\n           Conv2d-12         [-1, 64, 125, 125]          36,864\n      BatchNorm2d-13         [-1, 64, 125, 125]             128\n             ReLU-14         [-1, 64, 125, 125]               0\n           Conv2d-15         [-1, 64, 125, 125]          36,864\n      BatchNorm2d-16         [-1, 64, 125, 125]             128\n             ReLU-17         [-1, 64, 125, 125]               0\n       BasicBlock-18         [-1, 64, 125, 125]               0\n           Conv2d-19         [-1, 84, 125, 125]          48,468\n           Conv2d-20         [-1, 16, 125, 125]           9,232\n           Conv2d-21          [-1, 128, 63, 63]          73,728\n      BatchNorm2d-22          [-1, 128, 63, 63]             256\n             ReLU-23          [-1, 128, 63, 63]               0\n           Conv2d-24          [-1, 128, 63, 63]         147,456\n      BatchNorm2d-25          [-1, 128, 63, 63]             256\n           Conv2d-26          [-1, 128, 63, 63]           8,192\n      BatchNorm2d-27          [-1, 128, 63, 63]             256\n             ReLU-28          [-1, 128, 63, 63]               0\n       BasicBlock-29          [-1, 128, 63, 63]               0\n           Conv2d-30          [-1, 128, 63, 63]         147,456\n      BatchNorm2d-31          [-1, 128, 63, 63]             256\n             ReLU-32          [-1, 128, 63, 63]               0\n           Conv2d-33          [-1, 128, 63, 63]         147,456\n      BatchNorm2d-34          [-1, 128, 63, 63]             256\n             ReLU-35          [-1, 128, 63, 63]               0\n       BasicBlock-36          [-1, 128, 63, 63]               0\n           Conv2d-37           [-1, 84, 63, 63]          96,852\n           Conv2d-38           [-1, 16, 63, 63]          18,448\n           Conv2d-39          [-1, 256, 32, 32]         294,912\n      BatchNorm2d-40          [-1, 256, 32, 32]             512\n             ReLU-41          [-1, 256, 32, 32]               0\n           Conv2d-42          [-1, 256, 32, 32]         589,824\n      BatchNorm2d-43          [-1, 256, 32, 32]             512\n           Conv2d-44          [-1, 256, 32, 32]          32,768\n      BatchNorm2d-45          [-1, 256, 32, 32]             512\n             ReLU-46          [-1, 256, 32, 32]               0\n       BasicBlock-47          [-1, 256, 32, 32]               0\n           Conv2d-48          [-1, 256, 32, 32]         589,824\n      BatchNorm2d-49          [-1, 256, 32, 32]             512\n             ReLU-50          [-1, 256, 32, 32]               0\n           Conv2d-51          [-1, 256, 32, 32]         589,824\n      BatchNorm2d-52          [-1, 256, 32, 32]             512\n             ReLU-53          [-1, 256, 32, 32]               0\n       BasicBlock-54          [-1, 256, 32, 32]               0\n           Conv2d-55           [-1, 84, 32, 32]         193,620\n           Conv2d-56           [-1, 16, 32, 32]          36,880\n           Conv2d-57          [-1, 512, 16, 16]       1,179,648\n      BatchNorm2d-58          [-1, 512, 16, 16]           1,024\n             ReLU-59          [-1, 512, 16, 16]               0\n           Conv2d-60          [-1, 512, 16, 16]       2,359,296\n      BatchNorm2d-61          [-1, 512, 16, 16]           1,024\n           Conv2d-62          [-1, 512, 16, 16]         131,072\n      BatchNorm2d-63          [-1, 512, 16, 16]           1,024\n             ReLU-64          [-1, 512, 16, 16]               0\n       BasicBlock-65          [-1, 512, 16, 16]               0\n           Conv2d-66          [-1, 512, 16, 16]       2,359,296\n      BatchNorm2d-67          [-1, 512, 16, 16]           1,024\n             ReLU-68          [-1, 512, 16, 16]               0\n           Conv2d-69          [-1, 512, 16, 16]       2,359,296\n      BatchNorm2d-70          [-1, 512, 16, 16]           1,024\n             ReLU-71          [-1, 512, 16, 16]               0\n       BasicBlock-72          [-1, 512, 16, 16]               0\n           Conv2d-73           [-1, 84, 16, 16]         387,156\n           Conv2d-74           [-1, 16, 16, 16]          73,744\nAdaptiveMaxPool2d-75            [-1, 512, 1, 1]               0\n           Conv2d-76             [-1, 84, 1, 1]         387,156\n           Conv2d-77             [-1, 16, 1, 1]          73,744\n================================================================\nTotal params: 12,501,812\nTrainable params: 12,501,812\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 2.86\nForward/backward pass size (MB): 331.94\nParams size (MB): 47.69\nEstimated Total Size (MB): 382.49\n----------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"def box_iou(boxes1, boxes2):\n    \"\"\"Compute pairwise IoU across two lists of anchor or bounding boxes.\"\"\"\n    box_area = lambda boxes: ((boxes[:, 2] - boxes[:, 0]) *\n                              (boxes[:, 3] - boxes[:, 1]))\n    # Shape of `boxes1`, `boxes2`, `areas1`, `areas2`: (no. of boxes1, 4),\n    # (no. of boxes2, 4), (no. of boxes1,), (no. of boxes2,)\n    areas1 = box_area(boxes1)\n    areas2 = box_area(boxes2)\n    # Shape of `inter_upperlefts`, `inter_lowerrights`, `inters`: (no. of\n    # boxes1, no. of boxes2, 2)\n    inter_upperlefts = torch.max(boxes1[:, None, :2], boxes2[:, :2])\n    inter_lowerrights = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])\n    inters = (inter_lowerrights - inter_upperlefts).clamp(min=0)\n    # Shape of `inter_areas` and `union_areas`: (no. of boxes1, no. of boxes2)\n    inter_areas = inters[:, :, 0] * inters[:, :, 1]\n    union_areas = areas1[:, None] + areas2 - inter_areas\n    return inter_areas / union_areas","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:17:53.003915Z","iopub.execute_input":"2024-09-19T11:17:53.004231Z","iopub.status.idle":"2024-09-19T11:17:53.011736Z","shell.execute_reply.started":"2024-09-19T11:17:53.004197Z","shell.execute_reply":"2024-09-19T11:17:53.010828Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def assign_anchor_to_bbox(ground_truth, anchors, device, iou_threshold=0.5):\n    \"\"\"Assign closest ground-truth bounding boxes to anchor boxes.\"\"\"\n    num_anchors, num_gt_boxes = anchors.shape[0], ground_truth.shape[0]\n    # Element x_ij in the i-th row and j-th column is the IoU of the anchor\n    # box i and the ground-truth bounding box j\n    jaccard = box_iou(anchors, ground_truth)\n    # Initialize the tensor to hold the assigned ground-truth bounding box for\n    # each anchor\n    anchors_bbox_map = torch.full((num_anchors,), -1, dtype=torch.long,\n                                  device=device)\n    # Assign ground-truth bounding boxes according to the threshold\n    max_ious, indices = torch.max(jaccard, dim=1)\n    anc_i = torch.nonzero(max_ious >= iou_threshold).reshape(-1)\n    box_j = indices[max_ious >= iou_threshold]\n    anchors_bbox_map[anc_i] = box_j\n    col_discard = torch.full((num_anchors,), -1)\n    row_discard = torch.full((num_gt_boxes,), -1)\n    for _ in range(num_gt_boxes):\n        max_idx = torch.argmax(jaccard)  # Find the largest IoU\n        box_idx = (max_idx % num_gt_boxes).long()\n        anc_idx = (max_idx / num_gt_boxes).long()\n        anchors_bbox_map[anc_idx] = box_idx\n        jaccard[:, box_idx] = col_discard\n        jaccard[anc_idx, :] = row_discard\n    return anchors_bbox_map","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:17:53.013178Z","iopub.execute_input":"2024-09-19T11:17:53.013867Z","iopub.status.idle":"2024-09-19T11:17:53.022875Z","shell.execute_reply.started":"2024-09-19T11:17:53.013822Z","shell.execute_reply":"2024-09-19T11:17:53.021873Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def box_corner_to_center(boxes):\n    \"\"\" Convert from (upper-left, lower-right) to (center, width, height).\"\"\"\n    x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:,3]\n    cx = (x1 + x2) / 2\n    cy = (y1 + y2) / 2\n    w = x2 - x1\n    h = y2 - y1\n    boxes = torch.stack((cx, cy, w, h), axis=-1)\n    return boxes","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:17:53.023941Z","iopub.execute_input":"2024-09-19T11:17:53.024205Z","iopub.status.idle":"2024-09-19T11:17:53.036582Z","shell.execute_reply.started":"2024-09-19T11:17:53.024176Z","shell.execute_reply":"2024-09-19T11:17:53.035755Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def offset_boxes(anchors, assigned_bb, eps=1e-6):\n    \"\"\"Transform for anchor box offsets.\"\"\"\n    c_anc = box_corner_to_center(anchors)\n    c_assigned_bb = box_corner_to_center(assigned_bb)\n    offset_xy = 10 * (c_assigned_bb[:, :2] - c_anc[:, :2]) / c_anc[:, 2:]\n    offset_wh = 5 * torch.log(eps + c_assigned_bb[:, 2:] / c_anc[:, 2:])\n    offset = torch.cat([offset_xy, offset_wh], axis=1)\n    return offset","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:17:53.037568Z","iopub.execute_input":"2024-09-19T11:17:53.037882Z","iopub.status.idle":"2024-09-19T11:17:53.047116Z","shell.execute_reply.started":"2024-09-19T11:17:53.037849Z","shell.execute_reply":"2024-09-19T11:17:53.046199Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def multibox_target(anchors, labels):\n    \"\"\"Label anchor boxes using ground-truth bounding boxes.\"\"\"\n    batch_size, anchors = labels.shape[0], anchors.squeeze(0)\n    batch_offset, batch_mask, batch_class_labels = [], [], []\n    device, num_anchors = anchors.device, anchors.shape[0]\n    for i in range(batch_size):\n        label = labels[i, :, :]\n        anchors_bbox_map = assign_anchor_to_bbox(\n            label[:, 1:], anchors, device)\n        bbox_mask = ((anchors_bbox_map >= 0).float().unsqueeze(-1)).repeat(1, 4)\n        # Initialize class labels and assigned bounding box coordinates with\n        # zeros\n        class_labels = torch.zeros(num_anchors, dtype=torch.long,\n                                   device=device)\n        assigned_bb = torch.zeros((num_anchors, 4), dtype=torch.float32,\n                                  device=device)\n        # Label classes of anchor boxes using their assigned ground-truth\n        # bounding boxes. If an anchor box is not assigned any, we label its\n        # class as background (the value remains zero)\n        indices_true = torch.nonzero(anchors_bbox_map >= 0)\n        bb_idx = anchors_bbox_map[indices_true]\n        class_labels[indices_true] = label[bb_idx, 0].long() + 1\n        assigned_bb[indices_true] = label[bb_idx, 1:]\n        # Offset transformation\n        offset = offset_boxes(anchors, assigned_bb) * bbox_mask\n        batch_offset.append(offset.reshape(-1))\n        batch_mask.append(bbox_mask.reshape(-1))\n        batch_class_labels.append(class_labels)\n    bbox_offset = torch.stack(batch_offset)\n    bbox_mask = torch.stack(batch_mask)\n    class_labels = torch.stack(batch_class_labels)\n    return (bbox_offset, bbox_mask, class_labels)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:17:53.048296Z","iopub.execute_input":"2024-09-19T11:17:53.048993Z","iopub.status.idle":"2024-09-19T11:17:53.060959Z","shell.execute_reply.started":"2024-09-19T11:17:53.048950Z","shell.execute_reply":"2024-09-19T11:17:53.060111Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"trainer = torch.optim.NAdam(net.parameters(), lr=0.0001, weight_decay=5e-4)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:17:53.062141Z","iopub.execute_input":"2024-09-19T11:17:53.062453Z","iopub.status.idle":"2024-09-19T11:17:53.073898Z","shell.execute_reply.started":"2024-09-19T11:17:53.062405Z","shell.execute_reply":"2024-09-19T11:17:53.073092Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"cls_loss = nn.CrossEntropyLoss(reduction='sum')\nbbox_loss = nn.L1Loss(reduction='sum')\n\ndef calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks):\n    batch_size, num_classes = cls_preds.shape[0], cls_preds.shape[2]\n    # bbox_loss\n    bbox = bbox_loss(bbox_preds * bbox_masks,\n                     bbox_labels * bbox_masks)\n    cls_mask = bbox_masks.reshape(batch_size, -1, 4)\n    cls_mask = cls_mask.sum(dim = -1).reshape(-1)\n    foreground_idxs = torch.where(cls_mask > 0)[0]\n    num_foreground = foreground_idxs.shape[0]\n\n    background_idxs = torch.where(cls_mask == 0)[0]\n    keep_bg_idxs = background_idxs[torch.rand(*background_idxs.shape)>0.95]\n    cls_mask[keep_bg_idxs] = 4\n    cls = cls_loss(cls_preds.reshape(-1, num_classes)[cls_mask>0],\n                   cls_labels.reshape(-1)[cls_mask>0])\n    return cls/num_foreground, bbox/num_foreground","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:17:53.075051Z","iopub.execute_input":"2024-09-19T11:17:53.075451Z","iopub.status.idle":"2024-09-19T11:17:53.084271Z","shell.execute_reply.started":"2024-09-19T11:17:53.075419Z","shell.execute_reply":"2024-09-19T11:17:53.083408Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def cls_eval(cls_preds, cls_labels):\n    # Because the class prediction results are on the final dimension,\n    # `argmax` needs to specify this dimension\n    return float((cls_preds.argmax(dim=-1).type(\n        cls_labels.dtype) == cls_labels).sum())\n\ndef bbox_eval(bbox_preds, bbox_labels, bbox_masks):\n    return float((torch.abs((bbox_labels - bbox_preds) * bbox_masks)).sum())","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:17:53.085327Z","iopub.execute_input":"2024-09-19T11:17:53.085590Z","iopub.status.idle":"2024-09-19T11:17:53.097590Z","shell.execute_reply.started":"2024-09-19T11:17:53.085561Z","shell.execute_reply":"2024-09-19T11:17:53.096748Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"num_epochs, timer = 10, Timer()\nnet = net.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:17:53.102345Z","iopub.execute_input":"2024-09-19T11:17:53.102898Z","iopub.status.idle":"2024-09-19T11:17:53.108987Z","shell.execute_reply.started":"2024-09-19T11:17:53.102866Z","shell.execute_reply":"2024-09-19T11:17:53.108246Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"metric = {'train_loss': [],\n          'cls_loss': [],\n          'bbox_loss': []}\nfor epoch in range(num_epochs):\n    # Sum of training accuracy, no. of examples in sum of training accuracy,\n    # Sum of absolute error, no. of examples in sum of absolute error\n    net.train()\n    for features, target in train_dataloader:\n        timer.start()\n        trainer.zero_grad()\n        X, Y = features.to(device), target.to(device)\n        # Generate multiscale anchor boxes and predict their classes and\n        # offsets\n        anchors, cls_preds, bbox_preds = net(X)\n        # Label the classes and offsets of these anchor boxes\n        bbox_labels, bbox_masks, cls_labels = multibox_target(anchors, Y)\n        # Calculate the loss function using the predicted and labeled values\n        # of the classes and offsets\n        cls_l, bbox_l = calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels,\n                      bbox_masks)\n        l = 0.1*cls_l + 0.9*bbox_l\n        l.backward()\n        trainer.step()\n        metric['train_loss'].append(l.item())\n        metric['cls_loss'].append(cls_l.item())\n        metric['bbox_loss'].append(bbox_l.item())\n        # print('Loss: %.2f Cls_loss: %.2f Bbox_loss %.2f' %(l.item(), cls_l.item(), bbox_l.item()))\n    print(f'Epoch {epoch}: Loss: %.2f Cls_loss: %.2f Bbox_loss %.2f'%(sum(metric['train_loss'][-len(train_dataloader):])/len(train_dataloader),\n                                                                      sum(metric['cls_loss'][-len(train_dataloader):])/len(train_dataloader),\n                                                                      sum(metric['bbox_loss'][-len(train_dataloader):])/len(train_dataloader)))","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:49:54.511905Z","iopub.execute_input":"2024-09-19T11:49:54.512259Z","iopub.status.idle":"2024-09-19T12:44:19.522877Z","shell.execute_reply.started":"2024-09-19T11:49:54.512226Z","shell.execute_reply":"2024-09-19T12:44:19.521884Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/985353478.py:10: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  file_name = row[0]\n/tmp/ipykernel_36/985353478.py:13: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  bbox = row[1]\n/tmp/ipykernel_36/985353478.py:20: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  name = row[2]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0: Loss: 6887.14 Cls_loss: 2.76 Bbox_loss 7652.07\nEpoch 1: Loss: 6888.20 Cls_loss: 2.89 Bbox_loss 7653.23\nEpoch 2: Loss: 6888.06 Cls_loss: 3.03 Bbox_loss 7653.06\nEpoch 3: Loss: 6887.45 Cls_loss: 3.10 Bbox_loss 7652.38\nEpoch 4: Loss: 6888.34 Cls_loss: 3.07 Bbox_loss 7653.37\nEpoch 5: Loss: 6886.87 Cls_loss: 3.07 Bbox_loss 7651.73\nEpoch 6: Loss: 6886.46 Cls_loss: 3.05 Bbox_loss 7651.28\nEpoch 7: Loss: 6886.37 Cls_loss: 3.06 Bbox_loss 7651.18\nEpoch 8: Loss: 6884.96 Cls_loss: 3.06 Bbox_loss 7649.62\nEpoch 9: Loss: 6886.11 Cls_loss: 3.09 Bbox_loss 7650.89\n","output_type":"stream"}]}]}